{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a482ac7d",
   "metadata": {},
   "source": [
    "# Test des Intent Blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d1fef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from datasets import Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2372a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "f030485e-0f5d-43a7-8dfd-546903305127",
       "rows": [
        [
         "0",
         "Wie spät ist es?",
         "question"
        ],
        [
         "1",
         "Wer ist der aktuelle Bundespräsident?",
         "question"
        ],
        [
         "2",
         "Was ist die Hauptstadt von Australien?",
         "question"
        ],
        [
         "3",
         "Erzähl mir etwas über die Geschichte der Raumfahrt.",
         "question"
        ],
        [
         "4",
         "Wie funktioniert Photosynthese?",
         "question"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wie spät ist es?</td>\n",
       "      <td>question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wer ist der aktuelle Bundespräsident?</td>\n",
       "      <td>question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Was ist die Hauptstadt von Australien?</td>\n",
       "      <td>question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Erzähl mir etwas über die Geschichte der Raumf...</td>\n",
       "      <td>question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wie funktioniert Photosynthese?</td>\n",
       "      <td>question</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text     label\n",
       "0                                   Wie spät ist es?  question\n",
       "1              Wer ist der aktuelle Bundespräsident?  question\n",
       "2             Was ist die Hauptstadt von Australien?  question\n",
       "3  Erzähl mir etwas über die Geschichte der Raumf...  question\n",
       "4                    Wie funktioniert Photosynthese?  question"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/intent_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48bc01c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "222ed78a-7b6b-4d01-b4f1-b0707387d033",
       "rows": [
        [
         "text",
         "object"
        ],
        [
         "label",
         "object"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 2
       }
      },
      "text/plain": [
       "text     object\n",
       "label    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2232b2c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9dc54e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Häufigkeit der Labels im Datensatz:\n",
      "label\n",
      "question           50\n",
      "summary_request    50\n",
      "internet_search    50\n",
      "feedback           49\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "label_counts = df['label'].value_counts()\n",
    "print(\"Häufigkeit der Labels im Datensatz:\")\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f03144",
   "metadata": {},
   "source": [
    "Fine Tuning DistilBERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4412f31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label-Zuordnungen:\n",
      "String zu ID: {'question': 0, 'summary_request': 1, 'feedback': 2, 'internet_search': 3}\n",
      "ID zu String: {0: 'question', 1: 'summary_request', 2: 'feedback', 3: 'internet_search'}\n",
      "\n",
      "Trainingsdaten: 159 Beispiele\n",
      "Testdaten: 40 Beispiele\n",
      "\n",
      "Schritt 2: Tokenizer und Modell laden...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modell 'distilbert-base-multilingual-cased' und Tokenizer geladen.\n",
      "\n",
      "Schritt 3: Daten tokenisieren...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e52e45044e4a808a024317513122a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/159 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca7b8c7a587f4eeebc9ae47eb3aaa667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mauro\\AppData\\Local\\Temp\\ipykernel_17552\\2249414426.py:111: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daten tokenisiert und formatiert.\n",
      "\n",
      "Schritt 4: Metriken-Funktion definieren...\n",
      "Metriken-Funktion definiert.\n",
      "\n",
      "Schritt 5: Training konfigurieren und starten...\n",
      "Training startet...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 02:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.217100</td>\n",
       "      <td>1.078446</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.899749</td>\n",
       "      <td>0.904040</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.807300</td>\n",
       "      <td>0.694583</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.949875</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.635200</td>\n",
       "      <td>0.559012</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.950957</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training abgeschlossen.\n",
      "\n",
      "Schritt 6: Modell speichern in './my_german_intent_analyzer'...\n",
      "Modell und Tokenizer erfolgreich gespeichert.\n"
     ]
    }
   ],
   "source": [
    "# Labels numerisch kodieren und Zuordnungen erstellen\n",
    "# Dies ist wichtig, da das Modell mit numerischen IDs arbeitet\n",
    "unique_labels = df['label'].unique()\n",
    "label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
    "id_to_label = {i: label for i, label in enumerate(unique_labels)}\n",
    "\n",
    "df['label_id'] = df['label'].map(label_to_id)\n",
    "\n",
    "print(\"\\nLabel-Zuordnungen:\")\n",
    "print(\"String zu ID:\", label_to_id)\n",
    "print(\"ID zu String:\", id_to_label)\n",
    "\n",
    "# Daten in Trainings- und Testsets aufteilen\n",
    "# 'stratify=df['label']' sorgt dafür, dass die Verteilung der Labels in beiden Sets gleich ist\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label'])\n",
    "\n",
    "# Pandas DataFrames in Hugging Face Datasets konvertieren\n",
    "# 'drop(columns=['__index_level_0__'], errors='ignore')' ist notwendig, da Pandas einen temporären Index hinzufügen kann\n",
    "train_dataset = Dataset.from_pandas(train_df.drop(columns=['__index_level_0__'], errors='ignore'))\n",
    "test_dataset = Dataset.from_pandas(test_df.drop(columns=['__index_level_0__'], errors='ignore'))\n",
    "\n",
    "print(f\"\\nTrainingsdaten: {len(train_dataset)} Beispiele\")\n",
    "print(f\"Testdaten: {len(test_dataset)} Beispiele\")\n",
    "\n",
    "# --- 2. Tokenizer und Modell laden ---\n",
    "print(\"\\nSchritt 2: Tokenizer und Modell laden...\")\n",
    "# Wir verwenden ein multilinguales, cased (Groß-/Kleinschreibung beachtendes) DistilBERT Modell\n",
    "# Es ist gut für Deutsch geeignet und kleiner/schneller als das volle BERT.\n",
    "model_name = \"distilbert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# AutoModelForSequenceClassification lädt ein vortrainiertes Modell\n",
    "# und fügt automatisch einen Klassifikations-Layer für unsere 'num_labels' hinzu.\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(unique_labels), # Anzahl der Intents\n",
    "    id2label=id_to_label,          # Mapping von ID zu Label-String\n",
    "    label2id=label_to_id           # Mapping von Label-String zu ID\n",
    ")\n",
    "print(f\"Modell '{model_name}' und Tokenizer geladen.\")\n",
    "\n",
    "\n",
    "# --- 3. Daten tokenisieren ---\n",
    "print(\"\\nSchritt 3: Daten tokenisieren...\")\n",
    "def tokenize_function(examples):\n",
    "    # 'truncation=True' schneidet längere Texte ab, damit sie zur maximalen Länge des Modells passen\n",
    "    # 'padding=True' fügt kürzere Texte mit Füllzeichen auf die maximale Länge auf\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Spalte 'label_id' in 'labels' umbenennen, da der Hugging Face Trainer 'labels' erwartet\n",
    "tokenized_train_dataset = tokenized_train_dataset.rename_column(\"label_id\", \"labels\")\n",
    "tokenized_test_dataset = tokenized_test_dataset.rename_column(\"label_id\", \"labels\")\n",
    "\n",
    "# Ursprüngliche 'text' und 'label' Spalten entfernen, da das Modell die tokenisierten IDs benötigt\n",
    "tokenized_train_dataset = tokenized_train_dataset.remove_columns([\"text\", \"label\"])\n",
    "tokenized_test_dataset = tokenized_test_dataset.remove_columns([\"text\", \"label\"])\n",
    "\n",
    "# Stellen Sie sicher, dass die Labels vom Typ Long sind (wichtig für PyTorch/TensorFlow)\n",
    "tokenized_train_dataset.set_format(\"torch\")\n",
    "tokenized_test_dataset.set_format(\"torch\")\n",
    "\n",
    "print(\"Daten tokenisiert und formatiert.\")\n",
    "\n",
    "\n",
    "# --- 4. Metriken-Funktion definieren ---\n",
    "# Diese Funktion wird verwendet, um die Leistung des Modells während des Trainings zu bewerten\n",
    "print(\"\\nSchritt 4: Metriken-Funktion definieren...\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred # Logits sind die Rohergebnisse des Modells, Labels sind die wahren Klassen\n",
    "    predictions = np.argmax(logits, axis=-1) # Die Klasse mit dem höchsten Logit ist die Vorhersage\n",
    "\n",
    "    # Berechnung von Precision, Recall und F1-Score\n",
    "    # 'average='weighted'' berücksichtigt die Ungleichheit der Klassenverteilung\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted', zero_division=0)\n",
    "    acc = accuracy_score(labels, predictions) # Genauigkeit\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "print(\"Metriken-Funktion definiert.\")\n",
    "\n",
    "\n",
    "# --- 5. Training konfigurieren und starten ---\n",
    "print(\"\\nSchritt 5: Training konfigurieren und starten...\")\n",
    "output_dir = \"./my_german_intent_analyzer\" # Verzeichnis, in dem das Modell gespeichert wird\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "\t\t\t\tsave_strategy=\"epoch\",              # Ausgabe-Verzeichnis für Checkpoints und Ergebnisse\n",
    "    eval_strategy=\"epoch\",              # Bewertung nach jeder Epoche\n",
    "    learning_rate=2e-5,                 # Typische Lernrate für Fine-Tuning von Transformers\n",
    "    per_device_train_batch_size=8,      # Batch-Größe pro Gerät für Training\n",
    "    per_device_eval_batch_size=8,       # Batch-Größe pro Gerät für Bewertung\n",
    "    num_train_epochs=3,                 # Anzahl der Trainings-Epochen\n",
    "    weight_decay=0.01,                  # Regularisierung zur Vermeidung von Overfitting\n",
    "    save_total_limit=2,                 # Nur die 2 besten Checkpoints speichern\n",
    "    load_best_model_at_end=True,        # Bestes Modell (basierend auf eval_metric) am Ende laden\n",
    "    metric_for_best_model=\"f1\",         # Metrik, die zur Bestimmung des \"besten\" Modells verwendet wird\n",
    "    greater_is_better=True,             # Höherer F1-Score ist besser\n",
    "    logging_dir='./logs',               # Verzeichnis für TensorBoard-Logs\n",
    "    logging_steps=10,                   # Logs alle 10 Schritte ausgeben\n",
    "    report_to=\"none\"                    # Keine Berichte an Online-Dienste wie wandb\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_test_dataset,\n",
    "    tokenizer=tokenizer, # Tokenizer auch an den Trainer übergeben\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Starten des Trainings\n",
    "print(\"Training startet...\")\n",
    "trainer.train()\n",
    "print(\"Training abgeschlossen.\")\n",
    "\n",
    "\n",
    "# --- 6. Modell speichern ---\n",
    "# Das beste Modell (gemäß 'load_best_model_at_end=True') ist bereits im 'model'-Objekt geladen.\n",
    "# Wir speichern es nun im angegebenen Output-Verzeichnis.\n",
    "print(f\"\\nSchritt 6: Modell speichern in '{output_dir}'...\")\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir) # Auch den Tokenizer speichern!\n",
    "print(\"Modell und Tokenizer erfolgreich gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99c26b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Schritt 7: Finalen Testscore abrufen...\n",
      "\n",
      "--- Finaler Testscore ---\n",
      "eval_loss: 0.5590\n",
      "eval_accuracy: 0.9500\n",
      "eval_f1: 0.9510\n",
      "eval_precision: 0.9583\n",
      "eval_recall: 0.9500\n",
      "eval_runtime: 0.4376\n",
      "eval_samples_per_second: 91.4120\n",
      "eval_steps_per_second: 11.4270\n",
      "epoch: 3.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSchritt 7: Finalen Testscore abrufen...\")\n",
    "test_results = trainer.evaluate(eval_dataset=tokenized_test_dataset)\n",
    "\n",
    "print(\"\\n--- Finaler Testscore ---\")\n",
    "for metric_name, value in test_results.items():\n",
    "    print(f\"{metric_name}: {value:.4f}\")\n",
    "\n",
    "# Zusätzliche Ausgabe für richtige und falsche Vorhersagen\n",
    "if 'eval_correct_predictions' in test_results and 'eval_incorrect_predictions' in test_results:\n",
    "    print(f\"\\nRichtige Vorhersagen: {int(test_results['eval_correct_predictions'])} von {len(tokenized_test_dataset)}\")\n",
    "    print(f\"Falsche Vorhersagen: {int(test_results['eval_incorrect_predictions'])} von {len(tokenized_test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871eb27d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Agentvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
